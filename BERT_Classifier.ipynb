{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":[],"mount_file_id":"1bK5YGl99Q7JhHXgjYtQLEMyqpzh1XjB1","authorship_tag":"ABX9TyNWaWbNZk+krlnhZvUa6EEw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"E2IMwwMQuG1Y"},"source":["# **Multi Class Text Classification using BERT**\n","This notebook implements a multi class classifier using BERT, to predict the severity of a Reddit user's depression level.\n","\n","In order to create this code, I followed this tutorial https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613.\n","\n","This model is in response to, and uses the dataset from, task 3 of CLEF's eRisk 2021 workshop: https://early.irlab.org."]},{"cell_type":"markdown","metadata":{"id":"xwEht5gXwqyb"},"source":["# **Environment Set-up**"]},{"cell_type":"markdown","metadata":{"id":"dIbnl1TyfPFp"},"source":["### **Importing and Installing Required Libraries**"]},{"cell_type":"code","metadata":{"id":"deGtFgIHjtAA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629097090454,"user_tz":-60,"elapsed":3017,"user":{"displayName":"Erin Macfarlane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0_xwTZxkXsQUII-LehAPUB3M0y9Z6Xeb7Ef9CtQ=s64","userId":"04346088377799420888"}},"outputId":"7244a6e4-abd1-4902-a4a0-69f1efedffa4"},"source":["!pip install transformers\n","import torch\n","import pandas as pd\n","import numpy as np\n","import random\n","import csv\n","\n","from tqdm.notebook import tqdm\n","from transformers import RobertaTokenizer\n","from torch.utils.data import TensorDataset\n","from transformers import RobertaForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","\n","#change to the correct directory\n","%cd /content/drive/MyDrive/CS408/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","/content/drive/MyDrive/CS408\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sVqe71-PJJ4A"},"source":["# **Preparing Data**\n","The dataset has been preprocessed here: https://colab.research.google.com/drive/1J8XAD7JPShZQuBRJ6zfFjV1dUtH4y858?usp=sharing"]},{"cell_type":"code","metadata":{"id":"yK6tcbeaU__1"},"source":["bdi_questions_answers = pd.read_csv(\"BDI_csv.csv\")\n","\n","#list of all 21 question names in BDI i.e. ['Sadness', 'Pessimism', ...]\n","questions = bdi_questions_answers['Question'].unique()\n","\n","#list of lists where element contains all possible answers per question \n","answers = []\n","\n","#look up in bdi_questions_answers df for all possible answers per question and add to answers\n","for question_name in questions:\n","  answers.append(bdi_questions_answers.loc[((bdi_questions_answers['Question'] == question_name)), 'Answer'].values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwM5cy2E7GaV"},"source":["## **Create Dataframes**\n","\n","Convert the wrangled datset csvs into dataframes for ease of use later on. Creating variables and dataframes dynamically is bad practice, thus I made the csvs first and convert them here into their respective dataframes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Omph4xzTpOkQ","executionInfo":{"status":"ok","timestamp":1629073541580,"user_tz":-60,"elapsed":16,"user":{"displayName":"Erin Macfarlane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0_xwTZxkXsQUII-LehAPUB3M0y9Z6Xeb7Ef9CtQ=s64","userId":"04346088377799420888"}},"outputId":"38474284-50e1-46fb-8c2b-c40f6ba31b22"},"source":["%cd question_csvs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/CS408/question_csvs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HYZaqkhQrJTL"},"source":["#Creating data frames for each question \n","agitation_df = pd.read_csv(\"answer_classes_posts_Agitation.csv\") \n","appetite_df = pd.read_csv(\"answer_classes_posts_Changes in Appetite.csv\")\n","sleep_df = pd.read_csv(\"answer_classes_posts_Changes in Sleeping Pattern.csv\") \n","concentration_df = pd.read_csv(\"answer_classes_posts_Concentration Difficulty.csv\") \n","crying_df = pd.read_csv(\"answer_classes_posts_Crying.csv\") \n","guilty_df = pd.read_csv(\"answer_classes_posts_Guilty Feelings.csv\")\n","indecisive_df = pd.read_csv(\"answer_classes_posts_Indecisiveness.csv\") \n","irritability_df = pd.read_csv(\"answer_classes_posts_Irritability.csv\") \n","energy_df = pd.read_csv(\"answer_classes_posts_Loss of Energy.csv\") \n","sexinterest_df = pd.read_csv(\"answer_classes_posts_Loss of Interest in Sex.csv\") \n","interest_df = pd.read_csv(\"answer_classes_posts_Loss of Interest.csv\") \n","pleasure_df = pd.read_csv(\"answer_classes_posts_Loss of Pleasure.csv\") \n","pastfailure_df = pd.read_csv(\"answer_classes_posts_Past Failure.csv\") \n","pessimism_df = pd.read_csv(\"answer_classes_posts_Pessimism.csv\") \n","punishment_df = pd.read_csv(\"answer_classes_posts_Punishment Feelings.csv\") \n","sadness_df = pd.read_csv(\"answer_classes_posts_Sadness.csv\") \n","selfcritcalness_df = pd.read_csv(\"answer_classes_posts_Self-Criticalness.csv\") \n","selfdislike_df = pd.read_csv(\"answer_classes_posts_Self-Dislike.csv\") \n","suicidal_df = pd.read_csv(\"answer_classes_posts_Suicidal Thoughts or Wishes.csv\") \n","fatigue_df = pd.read_csv(\"answer_classes_posts_Tiredness or Fatigue.csv\") \n","worthlessness_df = pd.read_csv(\"answer_classes_posts_Worthlessness.csv\") \n","\n","#Create a list of the subject names as strings \n","# (any df could be used for this)\n","subjects = agitation_df['Subject'].unique()\n","\n","#Add all dataframes in order of BDI questions \n","BDI_df = [sadness_df, pessimism_df, pastfailure_df, pleasure_df, guilty_df, punishment_df, selfdislike_df, selfcritcalness_df, suicidal_df, crying_df, agitation_df, interest_df, indecisive_df, worthlessness_df, energy_df, sleep_df, irritability_df, appetite_df, concentration_df, fatigue_df, sexinterest_df]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpawc07LsUiF"},"source":["# **Create Classifier**"]},{"cell_type":"code","metadata":{"id":"xHq4grFRJxD6"},"source":["def evaluate(dataloader_test, model):\n","\n","    model.eval()\n","    \n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","    \n","    for batch in dataloader_test:\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():        \n","            outputs = model(**inputs)\n","            \n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","    \n","    loss_val_avg = loss_val_total/len(dataloader_test) \n","    \n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","            \n","    return loss_val_avg, predictions, true_vals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"myRdkhzTzWnP","executionInfo":{"status":"ok","timestamp":1629097935567,"user_tz":-60,"elapsed":321,"user":{"displayName":"Erin Macfarlane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0_xwTZxkXsQUII-LehAPUB3M0y9Z6Xeb7Ef9CtQ=s64","userId":"04346088377799420888"}},"outputId":"f648fb3f-e8b7-4d41-f352-a781cc499541"},"source":["%cd BERT_Classifier/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/CS408/BERT_Classifier\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H8S1sZYPWd6c"},"source":["for i, df in enumerate(BDI_df):\n","  \n","  label_dict = pd.Series(df.Class.values,index=df.Answer).to_dict()\n","  df['label'] = df.Answer.replace(label_dict)\n","\n","  #set data and labels \n","  X = df.index.values\n","  y = df.label\n","\n","  #test and train split\n","  X_train, X_test, y_train, y_test = train_test_split(df.index.values, \n","                                                    df.label.values, \n","                                                    test_size=0.30, \n","                                                    random_state=42, \n","                                                    stratify=df.label.values)\n","\n","  df['data_type'] = ['not_set']*df.shape[0]\n","\n","  df.loc[X_train, 'data_type'] = 'train'\n","  df.loc[X_test, 'data_type'] = 'test'\n","\n","  df.groupby(['Answer', 'label', 'data_type']).count()\n","\n","  dataset_train, dataset_test = build_and_encode()\n","  model = create_model()\n","  dataloader_train, dataloader_test = data_loaders(dataset_train, dataset_test)\n","  optimizer, scheduler = optimiser_and_scheduler(model)\n","\n","  seed_val = 17\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)\n","\n","  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","  model: model.to(device)\n","\n","  val_loss, predictions, true_vals = evaluate(dataloader_test, model)\n","\n","  #Training loop\n","  epochs = 5\n","  for epoch in tqdm(range(1, epochs+1)):\n","  \n","    model.train()\n","    \n","    loss_train_total = 0\n","\n","    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","    for batch in progress_bar:\n","        model.zero_grad()\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                }       \n","\n","        outputs = model(**inputs)\n","        \n","        loss = outputs[0]\n","        loss_train_total += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","    torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n","\n","    tqdm.write(f'\\nEpoch {epoch}')\n","    \n","    loss_train_avg = loss_train_total/len(dataloader_train)            \n","    tqdm.write(f'Training loss: {loss_train_avg}')\n","    \n","\n","    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",\n","                                                      num_labels=len(label_dict),\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","    model.to(device)\n","\n","    model.load_state_dict(torch.load('data_volume/finetuned_BERT_epoch_1.model', map_location=torch.device('cpu')))\n","\n","    #load and evaluate model\n","    _, predictions, true_vals = evaluate(dataloader_test, model)\n","\n","    #create dataframe storing the predicted answers\n","    resultsdf = pd.DataFrame(columns=['Class'], data=y_test)\n","    pred_flat = np.argmax(predictions, axis=1).flatten()\n","    resultsdf.insert(1, 'Predicted Class', value=pred_flat)\n","\n","    #sort the shuffled data back into place, \n","    # in order to match predicted answers with true class, subject and posts\n","    resultsdf.sort_index(inplace=True)\n","    sorted_indexes = resultsdf.index.to_list()\n","\n","    #create csv per question, making it easy to calculate evaluation metrics\n","    filename = questions[i] + '_results.csv'\n","    with open(filename, mode='w') as csv_file:\n","      csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","      csv_writer.writerow(['Subject', 'Post', 'Class', 'Predicted Class'])\n","      count = 0\n","      for j, row in df.iterrows():\n","        if count == len(sorted_indexes):\n","          break\n","        if (j == sorted_indexes[count]):\n","          subject = row['Subject']\n","          post = row['Post']\n","          real_class = resultsdf.loc[sorted_indexes[count], 'Class']\n","          pred_class = resultsdf.loc[sorted_indexes[count], 'Predicted Class']\n","          csv_writer.writerow([subject, post, real_class, pred_class])\n","          count += 1\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOxkeGm6rn06"},"source":["### **Constructs a RoBERTa tokenizer and instantiates a pre-trained RoBERTa model configuration to encode our data.**"]},{"cell_type":"code","metadata":{"id":"u6S4MXQ5Ba77"},"source":["def build_and_encode():\n","\n","  #create tokenizer\n","  tokenizer = RobertaTokenizer.from_pretrained('roberta-base', \n","                                            do_lower_case=True)\n","\n","  #encode training data                                         \n","  encoded_data_train = tokenizer.batch_encode_plus(\n","      df[df.data_type=='train'].Post.values, \n","      add_special_tokens=True, \n","      return_attention_mask=True, \n","      padding=True, \n","      max_length=128, \n","      return_tensors='pt',\n","      truncation=True\n","  )\n","\n","  #encode test data\n","  encoded_data_test = tokenizer.batch_encode_plus(\n","      df[df.data_type=='test'].Post.values, \n","      add_special_tokens=True, \n","      return_attention_mask=True, \n","      padding = True,\n","      max_length=128, \n","      return_tensors='pt',\n","      truncation=True\n","  )\n","\n","  #train\n","  input_ids_train = encoded_data_train['input_ids']\n","  attention_masks_train = encoded_data_train['attention_mask']\n","  labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n","\n","  #test\n","  input_ids_test = encoded_data_test['input_ids']\n","  attention_masks_test = encoded_data_test['attention_mask']\n","  labels_test = torch.tensor(df[df.data_type=='test'].label.values)\n","\n","  train_indexes = df[df.data_type=='train'].index.values.astype(int)\n","  test_indexes = df[df.data_type=='test'].index.values.astype(int)\n","\n","  dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","  dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n","\n","  return dataset_train, dataset_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nW519N9hrLVT"},"source":["### **Creates the RoBERTa pretrained model for classification**"]},{"cell_type":"code","metadata":{"id":"n5tNVoGHB2vf"},"source":["def create_model():\n","  model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",\n","                                                      num_labels=len(label_dict),\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JNerazeqVbz"},"source":["### **Create dataloaders that will combine a datset and sampler, providing an iterble over the data.**"]},{"cell_type":"code","metadata":{"id":"XXrP3q1MCA1u"},"source":["def data_loaders(dataset_train, dataset_test):\n","  \n","  from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","  batch_size = 3\n","\n","  dataloader_train = DataLoader(dataset_train, \n","                                sampler=RandomSampler(dataset_train), \n","                                batch_size=batch_size)\n","\n","  dataloader_test = DataLoader(dataset_test, \n","                                    sampler=SequentialSampler(dataset_test), \n","                                    batch_size=batch_size)\n","  \n","  return dataloader_train, dataloader_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ypZCUA4-poPx"},"source":["### **Constructs and optimiser and a scheduler**\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Gaw40cjVCEjB"},"source":["def optimiser_and_scheduler(model):\n","  from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","  #create optimiser\n","  optimizer = AdamW(model.parameters(),\n","                    lr=2e-5)\n","                    \n","  epochs = 4\n","\n","  #build scheduler\n","  scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                              num_warmup_steps=0,\n","                                              num_training_steps=len(dataloader_train)*epochs)\n","  \n","  return optimizer, scheduler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t7KnuvYrJxY3"},"source":["### **Results**\n","Unfortunately this model took too long to run and evaluate. So i was only able to gather predicted answers to posts for the first 3 questions. The question_hr function only calculates the number of times a post has been labeled correctly. Given more time, the answer would've been predicted per subject by counting the number of labels and seeing which one was predicted the most. The"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lq0-Gq0iHL2U","executionInfo":{"status":"ok","timestamp":1629103693538,"user_tz":-60,"elapsed":1398,"user":{"displayName":"Erin Macfarlane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0_xwTZxkXsQUII-LehAPUB3M0y9Z6Xeb7Ef9CtQ=s64","userId":"04346088377799420888"}},"outputId":"223cbe5b-120d-4881-b5ab-2d46c11f5e1a"},"source":["question_hr(pd.read_csv('Sadness_results.csv'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49.65"]},"metadata":{"tags":[]},"execution_count":160}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iIJnFACJb7M","executionInfo":{"status":"ok","timestamp":1629103732394,"user_tz":-60,"elapsed":1342,"user":{"displayName":"Erin Macfarlane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0_xwTZxkXsQUII-LehAPUB3M0y9Z6Xeb7Ef9CtQ=s64","userId":"04346088377799420888"}},"outputId":"097d86de-8b85-4bb3-862c-ccf89d966749"},"source":["question_hr(pd.read_csv('Pessimism_results.csv'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35.14"]},"metadata":{"tags":[]},"execution_count":161}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAmigRfkJfXG","executionInfo":{"status":"ok","timestamp":1629103733467,"user_tz":-60,"elapsed":1075,"user":{"displayName":"Erin Macfarlane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0_xwTZxkXsQUII-LehAPUB3M0y9Z6Xeb7Ef9CtQ=s64","userId":"04346088377799420888"}},"outputId":"feba68e2-8b6c-42ef-eab3-9c4c2072012b"},"source":["question_hr(pd.read_csv('Past Failure_results.csv'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["33.83"]},"metadata":{"tags":[]},"execution_count":162}]},{"cell_type":"code","metadata":{"id":"tQcNyPU1HZmX"},"source":["def question_hr(df):\n","  hr = 0\n","  for i, row in df.iterrows():\n","    if row['Class'] == row['Predicted Class']:\n","      hr += 1\n","    \n","  return round(hr/i * 100, 2) "],"execution_count":null,"outputs":[]}]}